<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>stpredictions.models.OK3 package &mdash; structured-predictions  documentation</title>
      <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="_static/css/custom.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="stpredictions.models.OK3.tests package" href="stpredictions.models.OK3.tests.html" />
    <link rel="prev" title="stpredictions.models.IOKR package" href="stpredictions.models.IOKR.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="index.html">
            <img src="_static/LogoTelecom.png" class="logo" alt="Logo"/>
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="modules.html">stpredictions</a><ul class="current">
<li class="toctree-l2 current"><a class="reference internal" href="stpredictions.html">stpredictions package</a><ul class="current">
<li class="toctree-l3 current"><a class="reference internal" href="stpredictions.html#subpackages">Subpackages</a><ul class="current">
<li class="toctree-l4"><a class="reference internal" href="stpredictions.datasets.html">stpredictions.datasets package</a></li>
<li class="toctree-l4"><a class="reference internal" href="stpredictions.helpers.html">stpredictions.helpers package</a></li>
<li class="toctree-l4"><a class="reference internal" href="stpredictions.metrics.html">stpredictions.metrics package</a></li>
<li class="toctree-l4 current"><a class="reference internal" href="stpredictions.models.html">stpredictions.models package</a><ul class="current">
<li class="toctree-l5 current"><a class="reference internal" href="stpredictions.models.html#subpackages">Subpackages</a><ul class="current">
<li class="toctree-l6"><a class="reference internal" href="stpredictions.models.DIOKR.html">stpredictions.models.DIOKR package</a></li>
<li class="toctree-l6"><a class="reference internal" href="stpredictions.models.IOKR.html">stpredictions.models.IOKR package</a></li>
<li class="toctree-l6 current"><a class="current reference internal" href="#">stpredictions.models.OK3 package</a><ul>
<li class="toctree-l7"><a class="reference internal" href="#subpackages">Subpackages</a></li>
<li class="toctree-l7"><a class="reference internal" href="#submodules">Submodules</a></li>
<li class="toctree-l7"><a class="reference internal" href="#module-stpredictions.models.OK3.base">stpredictions.models.OK3.base module</a></li>
<li class="toctree-l7"><a class="reference internal" href="#module-stpredictions.models.OK3.kernel">stpredictions.models.OK3.kernel module</a></li>
<li class="toctree-l7"><a class="reference internal" href="#module-stpredictions.models.OK3.structured_object">stpredictions.models.OK3.structured_object module</a></li>
<li class="toctree-l7"><a class="reference internal" href="#module-stpredictions.models.OK3">Module contents</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l5"><a class="reference internal" href="stpredictions.models.html#module-stpredictions.models">Module contents</a></li>
</ul>
</li>
<li class="toctree-l4"><a class="reference internal" href="stpredictions.viz.html">stpredictions.viz package</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="stpredictions.html#module-stpredictions">Module contents</a></li>
</ul>
</li>
</ul>
</li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">structured-predictions</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="modules.html">stpredictions</a> &raquo;</li>
          <li><a href="stpredictions.html">stpredictions package</a> &raquo;</li>
          <li><a href="stpredictions.models.html">stpredictions.models package</a> &raquo;</li>
      <li>stpredictions.models.OK3 package</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/stpredictions.models.OK3.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="stpredictions-models-ok3-package">
<h1>stpredictions.models.OK3 package<a class="headerlink" href="#stpredictions-models-ok3-package" title="Permalink to this headline"></a></h1>
<section id="subpackages">
<h2>Subpackages<a class="headerlink" href="#subpackages" title="Permalink to this headline"></a></h2>
<div class="toctree-wrapper compound">
<ul>
<li class="toctree-l1"><a class="reference internal" href="stpredictions.models.OK3.tests.html">stpredictions.models.OK3.tests package</a><ul>
<li class="toctree-l2"><a class="reference internal" href="stpredictions.models.OK3.tests.html#submodules">Submodules</a></li>
<li class="toctree-l2"><a class="reference internal" href="stpredictions.models.OK3.tests.html#stpredictions-models-ok3-tests-digits-image-completion-module">stpredictions.models.OK3.tests.digits_image_completion module</a></li>
<li class="toctree-l2"><a class="reference internal" href="stpredictions.models.OK3.tests.html#module-stpredictions.models.OK3.tests.exemple_utilisation">stpredictions.models.OK3.tests.exemple_utilisation module</a></li>
<li class="toctree-l2"><a class="reference internal" href="stpredictions.models.OK3.tests.html#module-stpredictions.models.OK3.tests.exemple_utilisation_forest">stpredictions.models.OK3.tests.exemple_utilisation_forest module</a></li>
<li class="toctree-l2"><a class="reference internal" href="stpredictions.models.OK3.tests.html#module-stpredictions.models.OK3.tests.test_classification">stpredictions.models.OK3.tests.test_classification module</a></li>
<li class="toctree-l2"><a class="reference internal" href="stpredictions.models.OK3.tests.html#module-stpredictions.models.OK3.tests.test_classification_forest">stpredictions.models.OK3.tests.test_classification_forest module</a></li>
<li class="toctree-l2"><a class="reference internal" href="stpredictions.models.OK3.tests.html#module-stpredictions.models.OK3.tests.test_export">stpredictions.models.OK3.tests.test_export module</a></li>
<li class="toctree-l2"><a class="reference internal" href="stpredictions.models.OK3.tests.html#module-stpredictions.models.OK3.tests.test_forest_clf_and_reg">stpredictions.models.OK3.tests.test_forest_clf_and_reg module</a></li>
<li class="toctree-l2"><a class="reference internal" href="stpredictions.models.OK3.tests.html#module-stpredictions.models.OK3.tests.test_regression">stpredictions.models.OK3.tests.test_regression module</a></li>
<li class="toctree-l2"><a class="reference internal" href="stpredictions.models.OK3.tests.html#module-stpredictions.models.OK3.tests.test_regression_forest">stpredictions.models.OK3.tests.test_regression_forest module</a></li>
<li class="toctree-l2"><a class="reference internal" href="stpredictions.models.OK3.tests.html#module-stpredictions.models.OK3.tests.test_tree_clf_and_reg">stpredictions.models.OK3.tests.test_tree_clf_and_reg module</a></li>
<li class="toctree-l2"><a class="reference internal" href="stpredictions.models.OK3.tests.html#module-stpredictions.models.OK3.tests">Module contents</a></li>
</ul>
</li>
</ul>
</div>
</section>
<section id="submodules">
<h2>Submodules<a class="headerlink" href="#submodules" title="Permalink to this headline"></a></h2>
</section>
<section id="module-stpredictions.models.OK3.base">
<span id="stpredictions-models-ok3-base-module"></span><h2>stpredictions.models.OK3.base module<a class="headerlink" href="#module-stpredictions.models.OK3.base" title="Permalink to this headline"></a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="stpredictions.models.OK3.base.StructuredOutputMixin">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">stpredictions.models.OK3.base.</span></span><span class="sig-name descname"><span class="pre">StructuredOutputMixin</span></span><a class="headerlink" href="#stpredictions.models.OK3.base.StructuredOutputMixin" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>Mixin to mark estimators that support structured prediction.</p>
</dd></dl>

</section>
<section id="module-stpredictions.models.OK3.kernel">
<span id="stpredictions-models-ok3-kernel-module"></span><h2>stpredictions.models.OK3.kernel module<a class="headerlink" href="#module-stpredictions.models.OK3.kernel" title="Permalink to this headline"></a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="stpredictions.models.OK3.kernel.Gaussian_Kernel">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">stpredictions.models.OK3.kernel.</span></span><span class="sig-name descname"><span class="pre">Gaussian_Kernel</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">gamma</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#stpredictions.models.OK3.kernel.Gaussian_Kernel" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="#stpredictions.models.OK3.kernel.Kernel" title="stpredictions.models.OK3.kernel.Kernel"><code class="xref py py-class docutils literal notranslate"><span class="pre">stpredictions.models.OK3.kernel.Kernel</span></code></a></p>
<dl class="py method">
<dt class="sig sig-object py" id="stpredictions.models.OK3.kernel.Gaussian_Kernel.evaluate">
<span class="sig-name descname"><span class="pre">evaluate</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">y1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y2</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#stpredictions.models.OK3.kernel.Gaussian_Kernel.evaluate" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stpredictions.models.OK3.kernel.Gaussian_Kernel.get_Gram_matrix">
<span class="sig-name descname"><span class="pre">get_Gram_matrix</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">list_y_1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">list_y_2</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#stpredictions.models.OK3.kernel.Gaussian_Kernel.get_Gram_matrix" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stpredictions.models.OK3.kernel.Gaussian_Kernel.get_sq_norms">
<span class="sig-name descname"><span class="pre">get_sq_norms</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">list_y</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#stpredictions.models.OK3.kernel.Gaussian_Kernel.get_sq_norms" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="stpredictions.models.OK3.kernel.Gini_Kernel">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">stpredictions.models.OK3.kernel.</span></span><span class="sig-name descname"><span class="pre">Gini_Kernel</span></span><a class="headerlink" href="#stpredictions.models.OK3.kernel.Gini_Kernel" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="#stpredictions.models.OK3.kernel.Mean_Dirac_Kernel" title="stpredictions.models.OK3.kernel.Mean_Dirac_Kernel"><code class="xref py py-class docutils literal notranslate"><span class="pre">stpredictions.models.OK3.kernel.Mean_Dirac_Kernel</span></code></a></p>
<p>Identique au ‘Mean_Dirac_Kernel’, mais permet de signaler que le décodage 
ne se fait pas parmi un candidates set mais est une recherche exhaustive.</p>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="stpredictions.models.OK3.kernel.Kernel">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">stpredictions.models.OK3.kernel.</span></span><span class="sig-name descname"><span class="pre">Kernel</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">name</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#stpredictions.models.OK3.kernel.Kernel" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<dl class="py method">
<dt class="sig sig-object py" id="stpredictions.models.OK3.kernel.Kernel.evaluate">
<span class="sig-name descname"><span class="pre">evaluate</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">obj1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">obj2</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#stpredictions.models.OK3.kernel.Kernel.evaluate" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stpredictions.models.OK3.kernel.Kernel.get_Gram_matrix">
<span class="sig-name descname"><span class="pre">get_Gram_matrix</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">objects_1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">objects_2</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#stpredictions.models.OK3.kernel.Kernel.get_Gram_matrix" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stpredictions.models.OK3.kernel.Kernel.get_name">
<span class="sig-name descname"><span class="pre">get_name</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#stpredictions.models.OK3.kernel.Kernel.get_name" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stpredictions.models.OK3.kernel.Kernel.get_sq_norms">
<span class="sig-name descname"><span class="pre">get_sq_norms</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">objects</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#stpredictions.models.OK3.kernel.Kernel.get_sq_norms" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="stpredictions.models.OK3.kernel.Laplacian_Kernel">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">stpredictions.models.OK3.kernel.</span></span><span class="sig-name descname"><span class="pre">Laplacian_Kernel</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">gamma</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#stpredictions.models.OK3.kernel.Laplacian_Kernel" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="#stpredictions.models.OK3.kernel.Kernel" title="stpredictions.models.OK3.kernel.Kernel"><code class="xref py py-class docutils literal notranslate"><span class="pre">stpredictions.models.OK3.kernel.Kernel</span></code></a></p>
<dl class="py method">
<dt class="sig sig-object py" id="stpredictions.models.OK3.kernel.Laplacian_Kernel.evaluate">
<span class="sig-name descname"><span class="pre">evaluate</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">y1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y2</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#stpredictions.models.OK3.kernel.Laplacian_Kernel.evaluate" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stpredictions.models.OK3.kernel.Laplacian_Kernel.get_Gram_matrix">
<span class="sig-name descname"><span class="pre">get_Gram_matrix</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">list_y_1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">list_y_2</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#stpredictions.models.OK3.kernel.Laplacian_Kernel.get_Gram_matrix" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stpredictions.models.OK3.kernel.Laplacian_Kernel.get_sq_norms">
<span class="sig-name descname"><span class="pre">get_sq_norms</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">list_y</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#stpredictions.models.OK3.kernel.Laplacian_Kernel.get_sq_norms" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="stpredictions.models.OK3.kernel.Linear_Kernel">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">stpredictions.models.OK3.kernel.</span></span><span class="sig-name descname"><span class="pre">Linear_Kernel</span></span><a class="headerlink" href="#stpredictions.models.OK3.kernel.Linear_Kernel" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="#stpredictions.models.OK3.kernel.Kernel" title="stpredictions.models.OK3.kernel.Kernel"><code class="xref py py-class docutils literal notranslate"><span class="pre">stpredictions.models.OK3.kernel.Kernel</span></code></a></p>
<dl class="py method">
<dt class="sig sig-object py" id="stpredictions.models.OK3.kernel.Linear_Kernel.evaluate">
<span class="sig-name descname"><span class="pre">evaluate</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">y1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y2</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#stpredictions.models.OK3.kernel.Linear_Kernel.evaluate" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stpredictions.models.OK3.kernel.Linear_Kernel.get_Gram_matrix">
<span class="sig-name descname"><span class="pre">get_Gram_matrix</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">list_y_1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">list_y_2</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#stpredictions.models.OK3.kernel.Linear_Kernel.get_Gram_matrix" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stpredictions.models.OK3.kernel.Linear_Kernel.get_sq_norms">
<span class="sig-name descname"><span class="pre">get_sq_norms</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">list_y</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#stpredictions.models.OK3.kernel.Linear_Kernel.get_sq_norms" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="stpredictions.models.OK3.kernel.MSE_Kernel">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">stpredictions.models.OK3.kernel.</span></span><span class="sig-name descname"><span class="pre">MSE_Kernel</span></span><a class="headerlink" href="#stpredictions.models.OK3.kernel.MSE_Kernel" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="#stpredictions.models.OK3.kernel.Linear_Kernel" title="stpredictions.models.OK3.kernel.Linear_Kernel"><code class="xref py py-class docutils literal notranslate"><span class="pre">stpredictions.models.OK3.kernel.Linear_Kernel</span></code></a></p>
<p>Identique au ‘Linear_Kernel’, mais permet de signaler que le décodage 
ne se fait pas parmi un candidates set mais est une solution exacte.</p>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="stpredictions.models.OK3.kernel.Mean_Dirac_Kernel">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">stpredictions.models.OK3.kernel.</span></span><span class="sig-name descname"><span class="pre">Mean_Dirac_Kernel</span></span><a class="headerlink" href="#stpredictions.models.OK3.kernel.Mean_Dirac_Kernel" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="#stpredictions.models.OK3.kernel.Kernel" title="stpredictions.models.OK3.kernel.Kernel"><code class="xref py py-class docutils literal notranslate"><span class="pre">stpredictions.models.OK3.kernel.Kernel</span></code></a></p>
<dl class="py method">
<dt class="sig sig-object py" id="stpredictions.models.OK3.kernel.Mean_Dirac_Kernel.evaluate">
<span class="sig-name descname"><span class="pre">evaluate</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">y1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y2</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#stpredictions.models.OK3.kernel.Mean_Dirac_Kernel.evaluate" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stpredictions.models.OK3.kernel.Mean_Dirac_Kernel.get_Gram_matrix">
<span class="sig-name descname"><span class="pre">get_Gram_matrix</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">list_y_1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">list_y_2</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#stpredictions.models.OK3.kernel.Mean_Dirac_Kernel.get_Gram_matrix" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stpredictions.models.OK3.kernel.Mean_Dirac_Kernel.get_sq_norms">
<span class="sig-name descname"><span class="pre">get_sq_norms</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">list_y</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#stpredictions.models.OK3.kernel.Mean_Dirac_Kernel.get_sq_norms" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="module-stpredictions.models.OK3.structured_object">
<span id="stpredictions-models-ok3-structured-object-module"></span><h2>stpredictions.models.OK3.structured_object module<a class="headerlink" href="#module-stpredictions.models.OK3.structured_object" title="Permalink to this headline"></a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="stpredictions.models.OK3.structured_object.StructuredObject">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">stpredictions.models.OK3.structured_object.</span></span><span class="sig-name descname"><span class="pre">StructuredObject</span></span><a class="headerlink" href="#stpredictions.models.OK3.structured_object.StructuredObject" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<dl class="py method">
<dt class="sig sig-object py" id="stpredictions.models.OK3.structured_object.StructuredObject.get_name">
<span class="sig-name descname"><span class="pre">get_name</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#stpredictions.models.OK3.structured_object.StructuredObject.get_name" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stpredictions.models.OK3.structured_object.StructuredObject.is_equal_to">
<span class="sig-name descname"><span class="pre">is_equal_to</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">other</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#stpredictions.models.OK3.structured_object.StructuredObject.is_equal_to" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stpredictions.models.OK3.structured_object.StructuredObject.similarity_with">
<span class="sig-name descname"><span class="pre">similarity_with</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">other</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kernel</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#stpredictions.models.OK3.structured_object.StructuredObject.similarity_with" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="module-stpredictions.models.OK3">
<span id="module-contents"></span><h2>Module contents<a class="headerlink" href="#module-stpredictions.models.OK3" title="Permalink to this headline"></a></h2>
<p>The <code class="xref py py-mod docutils literal notranslate"><span class="pre">sklearn.tree</span></code> module includes decision tree-based models for
classification and regression.</p>
<dl class="py class">
<dt class="sig sig-object py" id="stpredictions.models.OK3.BaseKernelizedOutputTree">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">stpredictions.models.OK3.</span></span><span class="sig-name descname"><span class="pre">BaseKernelizedOutputTree</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">criterion</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">splitter</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_depth</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_samples_split</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_samples_leaf</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_weight_fraction_leaf</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_features</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_leaf_nodes</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">random_state</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_impurity_decrease</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_impurity_split</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ccp_alpha</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kernel</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#stpredictions.models.OK3.BaseKernelizedOutputTree" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="#stpredictions.models.OK3.base.StructuredOutputMixin" title="stpredictions.models.OK3.base.StructuredOutputMixin"><code class="xref py py-class docutils literal notranslate"><span class="pre">stpredictions.models.OK3.base.StructuredOutputMixin</span></code></a>, <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.base.BaseEstimator</span></code></p>
<p>Base class for regression trees with a kernel in the output space.</p>
<p>Warning: This class should not be used directly.
Use derived classes instead.</p>
<dl class="py method">
<dt class="sig sig-object py" id="stpredictions.models.OK3.BaseKernelizedOutputTree.apply">
<span class="sig-name descname"><span class="pre">apply</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">check_input</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#stpredictions.models.OK3.BaseKernelizedOutputTree.apply" title="Permalink to this definition"></a></dt>
<dd><p>Return the index of the leaf that each sample is predicted as.</p>
<dl class="simple">
<dt>X<span class="classifier">{array-like, sparse matrix} of shape (n_samples, n_features)</span></dt><dd><p>The input samples. Internally, it will be converted to
<code class="docutils literal notranslate"><span class="pre">dtype=np.float32</span></code> and if a sparse matrix is provided
to a sparse <code class="docutils literal notranslate"><span class="pre">csr_matrix</span></code>.</p>
</dd>
<dt>check_input<span class="classifier">bool, default=True</span></dt><dd><p>Allow to bypass several input checking.
Don’t use this parameter unless you know what you do.</p>
</dd>
</dl>
<dl class="simple">
<dt>X_leaves<span class="classifier">array-like of shape (n_samples,)</span></dt><dd><p>For each datapoint x in X, return the index of the leaf x
ends up in. Leaves are numbered within
<code class="docutils literal notranslate"><span class="pre">[0;</span> <span class="pre">self.tree_.node_count)</span></code>, possibly with gaps in the
numbering.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stpredictions.models.OK3.BaseKernelizedOutputTree.cost_complexity_pruning_path">
<span class="sig-name descname"><span class="pre">cost_complexity_pruning_path</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sample_weight</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#stpredictions.models.OK3.BaseKernelizedOutputTree.cost_complexity_pruning_path" title="Permalink to this definition"></a></dt>
<dd><p>Compute the pruning path during Minimal Cost-Complexity Pruning.</p>
<p>See <span class="xref std std-ref">minimal_cost_complexity_pruning</span> for details on the pruning
process.</p>
<dl class="simple">
<dt>X<span class="classifier">{array-like, sparse matrix} of shape (n_samples, n_features)</span></dt><dd><p>The training input samples. Internally, it will be converted to
<code class="docutils literal notranslate"><span class="pre">dtype=np.float32</span></code> and if a sparse matrix is provided
to a sparse <code class="docutils literal notranslate"><span class="pre">csc_matrix</span></code>.</p>
</dd>
<dt>y<span class="classifier">array-like of shape (n_samples,) or (n_samples, n_outputs)</span></dt><dd><p>The target values (class labels) as integers or strings.</p>
</dd>
<dt>sample_weight<span class="classifier">array-like of shape (n_samples,), default=None</span></dt><dd><p>Sample weights. If None, then samples are equally weighted. Splits
that would create child nodes with net zero or negative weight are
ignored while searching for a split in each node. Splits are also
ignored if they would result in any single class carrying a
negative weight in either child node.</p>
</dd>
</dl>
<dl>
<dt>ccp_path<span class="classifier"><code class="xref py py-class docutils literal notranslate"><span class="pre">Bunch</span></code></span></dt><dd><p>Dictionary-like object, with the following attributes.</p>
<dl class="simple">
<dt>ccp_alphas<span class="classifier">ndarray</span></dt><dd><p>Effective alphas of subtree during pruning.</p>
</dd>
<dt>impurities<span class="classifier">ndarray</span></dt><dd><p>Sum of the impurities of the subtree leaves for the
corresponding alpha value in <code class="docutils literal notranslate"><span class="pre">ccp_alphas</span></code>.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stpredictions.models.OK3.BaseKernelizedOutputTree.decision_path">
<span class="sig-name descname"><span class="pre">decision_path</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">check_input</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#stpredictions.models.OK3.BaseKernelizedOutputTree.decision_path" title="Permalink to this definition"></a></dt>
<dd><p>Return the decision path in the tree.</p>
<dl class="simple">
<dt>X<span class="classifier">{array-like, sparse matrix} of shape (n_samples, n_features)</span></dt><dd><p>The input samples. Internally, it will be converted to
<code class="docutils literal notranslate"><span class="pre">dtype=np.float32</span></code> and if a sparse matrix is provided
to a sparse <code class="docutils literal notranslate"><span class="pre">csr_matrix</span></code>.</p>
</dd>
<dt>check_input<span class="classifier">bool, default=True</span></dt><dd><p>Allow to bypass several input checking.
Don’t use this parameter unless you know what you do.</p>
</dd>
</dl>
<dl class="simple">
<dt>indicator<span class="classifier">sparse matrix of shape (n_samples, n_nodes)</span></dt><dd><p>Return a node indicator CSR matrix where non zero elements
indicates that the samples goes through the nodes.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stpredictions.models.OK3.BaseKernelizedOutputTree.decode">
<span class="sig-name descname"><span class="pre">decode</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">candidates</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">check_input</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_top_k</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#stpredictions.models.OK3.BaseKernelizedOutputTree.decode" title="Permalink to this definition"></a></dt>
<dd><p>Synonyme de predict</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stpredictions.models.OK3.BaseKernelizedOutputTree.decode_tree">
<span class="sig-name descname"><span class="pre">decode_tree</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">candidates</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_top_k</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#stpredictions.models.OK3.BaseKernelizedOutputTree.decode_tree" title="Permalink to this definition"></a></dt>
<dd><p>Decode each leaves predictions of the tree, AND store the array of the decoded outputs
as an attribut of the estimator : self.leaves_preds.</p>
<p>ATTENTION, les prédictions correspondant aux noeuds qui ne sont pas des feuilles n’ont aucu  sens : elles sont arbitraires.
Elles n’ont volontairement pas été calculées pour question d’économie de temps.</p>
<dl class="simple">
<dt>candidates<span class="classifier">array of shape (nb_candidates, vectorial_repr_len), default=None</span></dt><dd><p>The candidates outputs for the minimisation problem of decoding the predictions
in the Hilbert space. 
If not given or None, it will be set to the output training matrix.</p>
</dd>
<dt>return_top_k<span class="classifier">int, default=1</span></dt><dd><p>Indicate how many decoded outputs to return for each example (or for each leaf).
Default is one : select the output that gives the  minimum “distance” with the 
predicted value in the Hilbert space. 
But it is useful to be able to return for example the 5 best candidates in order 
to evaluate a top 5 accuracy metric.</p>
</dd>
</dl>
<dl>
<dt>leaves_preds<span class="classifier">array-like of shape (node_count,vector_length)</span></dt><dd><p>For each leaf, return the vectorial representation of the output in ‘candidates’
that minimizes the “distance” with the “exact” prediction in the Hilbert space.</p>
<p>leaves_preds[i*return_top_k : (i+1)*return_top_k] is the non-ordered list od the 
decoded outputs of the node i among candidates.</p>
</dd>
</dl>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="stpredictions.models.OK3.BaseKernelizedOutputTree.feature_importances_">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">feature_importances_</span></span><a class="headerlink" href="#stpredictions.models.OK3.BaseKernelizedOutputTree.feature_importances_" title="Permalink to this definition"></a></dt>
<dd><p>Return the feature importances.</p>
<p>The importance of a feature is computed as the (normalized) total
reduction of the criterion brought by that feature.
It is also known as the Gini importance.</p>
<p>Warning: impurity-based feature importances can be misleading for
high cardinality features (many unique values). See
<code class="xref py py-func docutils literal notranslate"><span class="pre">sklearn.inspection.permutation_importance()</span></code> as an alternative.</p>
<dl class="simple">
<dt><a href="#id6"><span class="problematic" id="id7">feature_importances_</span></a><span class="classifier">ndarray of shape (n_features,)</span></dt><dd><p>Normalized total reduction of criteria by feature
(Gini importance).</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stpredictions.models.OK3.BaseKernelizedOutputTree.fit">
<span class="sig-name descname"><span class="pre">fit</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sample_weight</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">check_input</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">X_idx_sorted</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'deprecated'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kernel</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">in_ensemble</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">Gram_y</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#stpredictions.models.OK3.BaseKernelizedOutputTree.fit" title="Permalink to this definition"></a></dt>
<dd><dl>
<dt>kernel<span class="classifier">Optional input. If not provided, the kernel attribute of the</span></dt><dd><blockquote>
<div><p>estimatoris used. If provided, the kernel attribute of the estimator 
is updated.</p>
<p>Some possible values :</p>
</div></blockquote>
<dl>
<dt>“gini_clf”<span class="classifier">y is a matrix of labels for multilabel classification.</span></dt><dd><blockquote>
<div><p>shape (n_train_samples, n_labels)
We have to compute the corresponding gram matrix, 
equivalent to the use of a Classification Tree with the gini
index as impurity. Exact solution search is performed.</p>
</div></blockquote>
<dl class="simple">
<dt>“mse_reg”<span class="classifier">y is a matrix of real vectors for multiouput regression.</span></dt><dd><p>shape (n_train_samples, n_outputs)
We have to compute the corresponding gram matrix, 
equivalent to the use of a Regression Tree with the mse
as impurity. Exact solution search is performed.</p>
</dd>
<dt>“mean_dirac”<span class="classifier">y is a matrix or vectors (vectorial representation of structured objects).</span></dt><dd><p>shape (n_train_samples, vector_length)
The similarity between two objects is computed with a mean dirac equality kernel.</p>
</dd>
<dt>“linear”<span class="classifier">y is a matrix or vectors (vectorial representation of structured objects).</span></dt><dd><p>shape (n_train_samples, vector_length)
The similarity between two objects is computed with a linear kernel.</p>
</dd>
<dt>“gaussian”<span class="classifier">y is a matrix or vectors (vectorial representation of structured objects).</span></dt><dd><p>shape (n_train_samples, vector_length)
The similarity between two objects is computed with a gaussian kernel.</p>
</dd>
</dl>
</dd>
</dl>
</dd>
<dt>in_ensemble<span class="classifier">boolean, default=False</span></dt><dd><p>flag to set to true when the estimator is used with an ensemble method,
if True, the Gram matrix of the outputs is also given (as K_y) and doesn’t have to
be computed by the tree –&gt; avoid this heavy calculation for all trees.</p>
</dd>
<dt>Gram_y<span class="classifier">the output gram matrix, default=None</span></dt><dd><p>Allows to avoid the Gram matrix calculation if we already have it (useful when in_ensemble=True)</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stpredictions.models.OK3.BaseKernelizedOutputTree.get_depth">
<span class="sig-name descname"><span class="pre">get_depth</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#stpredictions.models.OK3.BaseKernelizedOutputTree.get_depth" title="Permalink to this definition"></a></dt>
<dd><p>Return the depth of the decision tree.</p>
<p>The depth of a tree is the maximum distance between the root
and any leaf.</p>
<dl class="simple">
<dt><a href="#id8"><span class="problematic" id="id9">self.tree_</span></a>.max_depth<span class="classifier">int</span></dt><dd><p>The maximum depth of the tree.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stpredictions.models.OK3.BaseKernelizedOutputTree.get_leaves_weights">
<span class="sig-name descname"><span class="pre">get_leaves_weights</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#stpredictions.models.OK3.BaseKernelizedOutputTree.get_leaves_weights" title="Permalink to this definition"></a></dt>
<dd><p>Gives the weighted training samples in each leaf</p>
<p>A (n_nodes, n_training_samples) array which gives for each node (line number)
and for each training sample its weight in the node (O if the sample doesn’t fall
in the node, and a non-negative value depending on ‘sample_weight’ otherwise.)</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stpredictions.models.OK3.BaseKernelizedOutputTree.get_n_leaves">
<span class="sig-name descname"><span class="pre">get_n_leaves</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#stpredictions.models.OK3.BaseKernelizedOutputTree.get_n_leaves" title="Permalink to this definition"></a></dt>
<dd><p>Return the number of leaves of the decision tree.</p>
<dl class="simple">
<dt><a href="#id10"><span class="problematic" id="id11">self.tree_</span></a>.n_leaves<span class="classifier">int</span></dt><dd><p>Number of leaves.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stpredictions.models.OK3.BaseKernelizedOutputTree.predict">
<span class="sig-name descname"><span class="pre">predict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">candidates</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">check_input</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_top_k</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#stpredictions.models.OK3.BaseKernelizedOutputTree.predict" title="Permalink to this definition"></a></dt>
<dd><p>Predict structured objects for X.</p>
<p>The predicted structured objects based on X are returned.
Performs an argmin research algorithm amongst the possible outputs</p>
<dl class="simple">
<dt>X<span class="classifier">{array-like, sparse matrix} of shape (n_samples, n_features)</span></dt><dd><p>The input samples. Internally, it will be converted to
<code class="docutils literal notranslate"><span class="pre">dtype=np.float32</span></code> and if a sparse matrix is provided
to a sparse <code class="docutils literal notranslate"><span class="pre">csr_matrix</span></code>.</p>
</dd>
<dt>candidates<span class="classifier">array of shape (nb_candidates, vectorial_repr_len), default=None</span></dt><dd><p>The candidates outputs for the minimisation problem of decoding the predictions
in the Hilbert space. 
If not given or None, it will be set to the output training matrix.</p>
</dd>
<dt>check_input<span class="classifier">bool, default=True</span></dt><dd><p>Allow to bypass several input checking.
Don’t use this parameter unless you know what you do.</p>
</dd>
<dt>return_top_k<span class="classifier">int, default=1</span></dt><dd><p>Indicate how many decoded outputs to return for each example (or for each leaf).
Default is one : select the output that gives the  minimum “distance” with the 
predicted value in the Hilbert space. 
But it is useful to be able to return for example the 5 best candidates in order 
to evaluate a top 5 accuracy metric.</p>
</dd>
</dl>
<dl class="simple">
<dt>output<span class="classifier"></span></dt><dd><p>array of shape (n_samples, vectorial_repr_len) containing the vectorial 
representations of the structured output objects (found in the set of candidates, 
or if it is not given, among the training outputs).</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stpredictions.models.OK3.BaseKernelizedOutputTree.predict_weights">
<span class="sig-name descname"><span class="pre">predict_weights</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">check_input</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#stpredictions.models.OK3.BaseKernelizedOutputTree.predict_weights" title="Permalink to this definition"></a></dt>
<dd><p>Predict the output for X as weighted combinations of training outputs
It is kind of the representation in the Hilbert space.</p>
<p>A (len(X), n_training_samples) array which gives for each test example (line number)
and for each training sample its weight in the node (O if the sample doesn’t fall
in the same leaf as the test example, and a non-negative value depending on ‘sample_weight’ otherwise.)</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stpredictions.models.OK3.BaseKernelizedOutputTree.r2_score_in_Hilbert">
<span class="sig-name descname"><span class="pre">r2_score_in_Hilbert</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sample_weight</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#stpredictions.models.OK3.BaseKernelizedOutputTree.r2_score_in_Hilbert" title="Permalink to this definition"></a></dt>
<dd><p>Calcule le score R2 SANS décodage</p>
<p>Return the coefficient of determination R^2 of the prediction in the Hilbert space.
The coefficient R^2 is defined as (1 - u/v), where u is the residual
sum of squares ((y_true - y_pred) ** 2).sum() and v is the total
sum of squares ((y_true - y_true.mean()) ** 2).sum().</p>
<dl class="simple">
<dt>X<span class="classifier">array-like of shape (n_samples, n_features)</span></dt><dd><p>Test samples.</p>
</dd>
<dt>y<span class="classifier">array-like of shape (n_samples,) or (n_samples, n_outputs)</span></dt><dd><p>True outputs for X.</p>
</dd>
<dt>sample_weight<span class="classifier">array-like of shape (n_samples,), default=None</span></dt><dd><p>Sample weights.</p>
</dd>
</dl>
<dl class="simple">
<dt>score<span class="classifier">float</span></dt><dd><p>R2 score of the predictions in the Hilbert space wrt. the embedded values of y.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stpredictions.models.OK3.BaseKernelizedOutputTree.score">
<span class="sig-name descname"><span class="pre">score</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">candidates</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">metric</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'accuracy'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sample_weight</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#stpredictions.models.OK3.BaseKernelizedOutputTree.score" title="Permalink to this definition"></a></dt>
<dd><p>Calcule le score après décodage</p>
<p>Return either</p>
<blockquote>
<div><p>-the coefficient of determination R^2 of the prediction.</p>
</div></blockquote>
<p>The coefficient R^2 is defined as (1 - u/v), where u is the residual
sum of squares ((y_true - y_pred) ** 2).sum() and v is the total
sum of squares ((y_true - y_true.mean()) ** 2).sum().</p>
<p>(IF self.kernel=”mse_reg”)</p>
<blockquote>
<div><p>-the mean accuracy of the predictions if metric=”accuracy”.
(Requires that all labels match to count as positive in case of multilabel)</p>
<p>-the mean hamming score of the predictions if metric=”hamming”
(Well suited for multilabel classification)</p>
<p>-the mean top k accuracy score if metric=”<a href="#id12"><span class="problematic" id="id13">top_</span></a>”+str(k)
It works with all wanted value of k.</p>
</div></blockquote>
<p>It is possible to set the ‘sample_weight’ parameter for all these metrics.</p>
<p>All this score metrics are highly dependent on the candidates set because it
deals with the decoded predictions (which are among this set).
If you want to compute a score only based on the tree structure, you can
use the following method ‘r2_score_in_Hilbert’.</p>
<dl class="simple">
<dt>X<span class="classifier">array-like of shape (n_samples, n_features)</span></dt><dd><p>Test samples.</p>
</dd>
<dt>y<span class="classifier">array-like of shape (n_samples,) or (n_samples, n_outputs)</span></dt><dd><p>True outputs for X.</p>
</dd>
<dt>candidates<span class="classifier">array-like of shape (nb_candidates, n_outputs)</span></dt><dd><p>Possible decoded outputs for X</p>
</dd>
<dt>metric<span class="classifier">str, default=”accuracy”</span></dt><dd><p>The way to compute the score</p>
</dd>
<dt>sample_weight<span class="classifier">array-like of shape (n_samples,), default=None</span></dt><dd><p>Sample weights.</p>
</dd>
</dl>
<dl class="simple">
<dt>score<span class="classifier">float</span></dt><dd><p>Chosen score between self.predict(X) and y.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="stpredictions.models.OK3.BaseOKForest">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">stpredictions.models.OK3.</span></span><span class="sig-name descname"><span class="pre">BaseOKForest</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">base_estimator</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_estimators</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">100</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">estimator_params</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">()</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bootstrap</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">oob_score</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_jobs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">random_state</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">warm_start</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_samples</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kernel</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'linear'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#stpredictions.models.OK3.BaseOKForest" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="#stpredictions.models.OK3.base.StructuredOutputMixin" title="stpredictions.models.OK3.base.StructuredOutputMixin"><code class="xref py py-class docutils literal notranslate"><span class="pre">stpredictions.models.OK3.base.StructuredOutputMixin</span></code></a>, <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.ensemble._base.BaseEnsemble</span></code></p>
<p>Base class for forests of ok-trees.</p>
<p>Warning: This class should not be used directly. Use derived classes
instead.</p>
<dl class="py method">
<dt class="sig sig-object py" id="stpredictions.models.OK3.BaseOKForest.apply">
<span class="sig-name descname"><span class="pre">apply</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#stpredictions.models.OK3.BaseOKForest.apply" title="Permalink to this definition"></a></dt>
<dd><p>Apply trees in the forest to X, return leaf indices.</p>
<dl class="simple">
<dt>X<span class="classifier">{array-like, sparse matrix} of shape (n_samples, n_features)</span></dt><dd><p>The input samples. Internally, its dtype will be converted to
<code class="docutils literal notranslate"><span class="pre">dtype=np.float32</span></code>. If a sparse matrix is provided, it will be
converted into a sparse <code class="docutils literal notranslate"><span class="pre">csr_matrix</span></code>.</p>
</dd>
</dl>
<dl class="simple">
<dt>X_leaves<span class="classifier">ndarray of shape (n_samples, n_estimators)</span></dt><dd><p>For each datapoint x in X and for each tree in the forest,
return the index of the leaf x ends up in.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stpredictions.models.OK3.BaseOKForest.decision_path">
<span class="sig-name descname"><span class="pre">decision_path</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#stpredictions.models.OK3.BaseOKForest.decision_path" title="Permalink to this definition"></a></dt>
<dd><p>Return the decision path in the forest.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.18.</span></p>
</div>
<dl class="simple">
<dt>X<span class="classifier">{array-like, sparse matrix} of shape (n_samples, n_features)</span></dt><dd><p>The input samples. Internally, its dtype will be converted to
<code class="docutils literal notranslate"><span class="pre">dtype=np.float32</span></code>. If a sparse matrix is provided, it will be
converted into a sparse <code class="docutils literal notranslate"><span class="pre">csr_matrix</span></code>.</p>
</dd>
</dl>
<dl class="simple">
<dt>indicator<span class="classifier">sparse matrix of shape (n_samples, n_nodes)</span></dt><dd><p>Return a node indicator matrix where non zero elements indicates
that the samples goes through the nodes. The matrix is of CSR
format.</p>
</dd>
<dt>n_nodes_ptr<span class="classifier">ndarray of shape (n_estimators + 1,)</span></dt><dd><p>The columns from indicator[n_nodes_ptr[i]:n_nodes_ptr[i+1]]
gives the indicator value for the i-th estimator.</p>
</dd>
</dl>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="stpredictions.models.OK3.BaseOKForest.feature_importances_">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">feature_importances_</span></span><a class="headerlink" href="#stpredictions.models.OK3.BaseOKForest.feature_importances_" title="Permalink to this definition"></a></dt>
<dd><p>The impurity-based feature importances.</p>
<p>The higher, the more important the feature.
The importance of a feature is computed as the (normalized)
total reduction of the criterion brought by that feature.  It is also
known as the Gini importance.</p>
<p>Warning: impurity-based feature importances can be misleading for
high cardinality features (many unique values). See
<code class="xref py py-func docutils literal notranslate"><span class="pre">sklearn.inspection.permutation_importance()</span></code> as an alternative.</p>
<dl class="simple">
<dt><a href="#id14"><span class="problematic" id="id15">feature_importances_</span></a><span class="classifier">ndarray of shape (n_features,)</span></dt><dd><p>The values of this array sum to 1, unless all trees are single node
trees consisting of only the root node, in which case it will be an
array of zeros.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stpredictions.models.OK3.BaseOKForest.fit">
<span class="sig-name descname"><span class="pre">fit</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sample_weight</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#stpredictions.models.OK3.BaseOKForest.fit" title="Permalink to this definition"></a></dt>
<dd><p>Build a forest of ok-trees from the training set (X, y).</p>
<dl class="simple">
<dt>X<span class="classifier">{array-like, sparse matrix} of shape (n_samples, n_features)</span></dt><dd><p>The training input samples. Internally, its dtype will be converted
to <code class="docutils literal notranslate"><span class="pre">dtype=np.float32</span></code>. If a sparse matrix is provided, it will be
converted into a sparse <code class="docutils literal notranslate"><span class="pre">csc_matrix</span></code>.</p>
</dd>
<dt>y<span class="classifier">array-like of shape (n_samples,) or (n_samples, n_outputs)</span></dt><dd><p>The target values (class labels in classification, real numbers in
regression).</p>
</dd>
<dt>sample_weight<span class="classifier">array-like of shape (n_samples,), default=None</span></dt><dd><p>Sample weights. If None, then samples are equally weighted. Splits
that would create child nodes with net zero or negative weight are
ignored while searching for a split in each node. In the case of
classification, splits are also ignored if they would result in any
single class carrying a negative weight in either child node.</p>
</dd>
</dl>
<p>self : object</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="stpredictions.models.OK3.ExtraOK3Regressor">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">stpredictions.models.OK3.</span></span><span class="sig-name descname"><span class="pre">ExtraOK3Regressor</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">criterion</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'mse'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">splitter</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'random'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_depth</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_samples_split</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_samples_leaf</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_weight_fraction_leaf</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_features</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'auto'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">random_state</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_impurity_decrease</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_impurity_split</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_leaf_nodes</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ccp_alpha</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kernel</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'linear'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#stpredictions.models.OK3.ExtraOK3Regressor" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="#stpredictions.models.OK3.OK3Regressor" title="stpredictions.models.OK3._classes.OK3Regressor"><code class="xref py py-class docutils literal notranslate"><span class="pre">stpredictions.models.OK3._classes.OK3Regressor</span></code></a></p>
<p>An extremely randomized tree regressor.</p>
<p>Extra-trees differ from classic decision trees in the way they are built.
When looking for the best split to separate the samples of a node into two
groups, random splits are drawn for each of the <cite>max_features</cite> randomly
selected features and the best split among those is chosen. When
<cite>max_features</cite> is set 1, this amounts to building a totally random
decision tree.</p>
<p>Warning: Extra-trees should only be used within ensemble methods.</p>
<dl>
<dt>criterion<span class="classifier">{“mse”, “friedman_mse”, “mae”}, default=”mse”</span></dt><dd><p>The function to measure the quality of a split. Supported criteria
are “mse” for the mean squared error, which is equal to variance
reduction as feature selection criterion, and “mae” for the mean
absolute error.</p>
</dd>
<dt>splitter<span class="classifier">{“random”, “best”}, default=”random”</span></dt><dd><p>The strategy used to choose the split at each node. Supported
strategies are “best” to choose the best split and “random” to choose
the best random split.</p>
</dd>
<dt>max_depth<span class="classifier">int, default=None</span></dt><dd><p>The maximum depth of the tree. If None, then nodes are expanded until
all leaves are pure or until all leaves contain less than
min_samples_split samples.</p>
</dd>
<dt>min_samples_split<span class="classifier">int or float, default=2</span></dt><dd><p>The minimum number of samples required to split an internal node:</p>
<ul class="simple">
<li><p>If int, then consider <cite>min_samples_split</cite> as the minimum number.</p></li>
<li><p>If float, then <cite>min_samples_split</cite> is a fraction and
<cite>ceil(min_samples_split * n_samples)</cite> are the minimum
number of samples for each split.</p></li>
</ul>
</dd>
<dt>min_samples_leaf<span class="classifier">int or float, default=1</span></dt><dd><p>The minimum number of samples required to be at a leaf node.
A split point at any depth will only be considered if it leaves at
least <code class="docutils literal notranslate"><span class="pre">min_samples_leaf</span></code> training samples in each of the left and
right branches.  This may have the effect of smoothing the model,
especially in regression.</p>
<ul class="simple">
<li><p>If int, then consider <cite>min_samples_leaf</cite> as the minimum number.</p></li>
<li><p>If float, then <cite>min_samples_leaf</cite> is a fraction and
<cite>ceil(min_samples_leaf * n_samples)</cite> are the minimum
number of samples for each node.</p></li>
</ul>
</dd>
<dt>min_weight_fraction_leaf<span class="classifier">float, default=0.0</span></dt><dd><p>The minimum weighted fraction of the sum total of weights (of all
the input samples) required to be at a leaf node. Samples have
equal weight when sample_weight is not provided.</p>
</dd>
<dt>max_features<span class="classifier">int, float, {“auto”, “sqrt”, “log2”} or None, default=”auto”</span></dt><dd><p>The number of features to consider when looking for the best split:</p>
<ul class="simple">
<li><p>If int, then consider <cite>max_features</cite> features at each split.</p></li>
<li><p>If float, then <cite>max_features</cite> is a fraction and
<cite>int(max_features * n_features)</cite> features are considered at each
split.</p></li>
<li><p>If “auto”, then <cite>max_features=n_features</cite>.</p></li>
<li><p>If “sqrt”, then <cite>max_features=sqrt(n_features)</cite>.</p></li>
<li><p>If “log2”, then <cite>max_features=log2(n_features)</cite>.</p></li>
<li><p>If None, then <cite>max_features=n_features</cite>.</p></li>
</ul>
<p>Note: the search for a split does not stop until at least one
valid partition of the node samples is found, even if it requires to
effectively inspect more than <code class="docutils literal notranslate"><span class="pre">max_features</span></code> features.</p>
</dd>
<dt>random_state<span class="classifier">int, RandomState instance, default=None</span></dt><dd><p>Used to pick randomly the <cite>max_features</cite> used at each split.
See <span class="xref std std-term">Glossary</span> for details.</p>
</dd>
<dt>min_impurity_decrease<span class="classifier">float, default=0.0</span></dt><dd><p>A node will be split if this split induces a decrease of the impurity
greater than or equal to this value.</p>
<p>The weighted impurity decrease equation is the following:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">N_t</span> <span class="o">/</span> <span class="n">N</span> <span class="o">*</span> <span class="p">(</span><span class="n">impurity</span> <span class="o">-</span> <span class="n">N_t_R</span> <span class="o">/</span> <span class="n">N_t</span> <span class="o">*</span> <span class="n">right_impurity</span>
                    <span class="o">-</span> <span class="n">N_t_L</span> <span class="o">/</span> <span class="n">N_t</span> <span class="o">*</span> <span class="n">left_impurity</span><span class="p">)</span>
</pre></div>
</div>
<p>where <code class="docutils literal notranslate"><span class="pre">N</span></code> is the total number of samples, <code class="docutils literal notranslate"><span class="pre">N_t</span></code> is the number of
samples at the current node, <code class="docutils literal notranslate"><span class="pre">N_t_L</span></code> is the number of samples in the
left child, and <code class="docutils literal notranslate"><span class="pre">N_t_R</span></code> is the number of samples in the right child.</p>
<p><code class="docutils literal notranslate"><span class="pre">N</span></code>, <code class="docutils literal notranslate"><span class="pre">N_t</span></code>, <code class="docutils literal notranslate"><span class="pre">N_t_R</span></code> and <code class="docutils literal notranslate"><span class="pre">N_t_L</span></code> all refer to the weighted sum,
if <code class="docutils literal notranslate"><span class="pre">sample_weight</span></code> is passed.</p>
</dd>
<dt>min_impurity_split<span class="classifier">float, (default=0)</span></dt><dd><p>Threshold for early stopping in tree growth. A node will split
if its impurity is above the threshold, otherwise it is a leaf.</p>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version 0.19: </span><code class="docutils literal notranslate"><span class="pre">min_impurity_split</span></code> has been deprecated in favor of
<code class="docutils literal notranslate"><span class="pre">min_impurity_decrease</span></code> in 0.19. The default value of
<code class="docutils literal notranslate"><span class="pre">min_impurity_split</span></code> has changed from 1e-7 to 0 in 0.23 and it
will be removed in 0.25. Use <code class="docutils literal notranslate"><span class="pre">min_impurity_decrease</span></code> instead.</p>
</div>
</dd>
<dt>max_leaf_nodes<span class="classifier">int, default=None</span></dt><dd><p>Grow a tree with <code class="docutils literal notranslate"><span class="pre">max_leaf_nodes</span></code> in best-first fashion.
Best nodes are defined as relative reduction in impurity.
If None then unlimited number of leaf nodes.</p>
</dd>
<dt>ccp_alpha<span class="classifier">non-negative float, default=0.0</span></dt><dd><p>Complexity parameter used for Minimal Cost-Complexity Pruning. The
subtree with the largest cost complexity that is smaller than
<code class="docutils literal notranslate"><span class="pre">ccp_alpha</span></code> will be chosen. By default, no pruning is performed. See
<span class="xref std std-ref">minimal_cost_complexity_pruning</span> for details.</p>
</dd>
<dt>kernel<span class="classifier">string, or tuple (string, params) or instance of the Kernel class, default=”linear”</span></dt><dd><p>The type of kernel to use to compare the output data. Changing this
parameter changes also implicitely the nature of the Hilbert space
in which the output data are embedded.
The string describes the type of Kernel to use (defined in Kernel.py), 
The optional params given are here to set particular parameters values
for the chosen kernel type.</p>
</dd>
</dl>
<dl>
<dt><a href="#id16"><span class="problematic" id="id17">max_features_</span></a><span class="classifier">int</span></dt><dd><p>The inferred value of max_features.</p>
</dd>
<dt><a href="#id18"><span class="problematic" id="id19">n_features_</span></a><span class="classifier">int</span></dt><dd><p>The number of features when <code class="docutils literal notranslate"><span class="pre">fit</span></code> is performed.</p>
</dd>
<dt><a href="#id20"><span class="problematic" id="id21">feature_importances_</span></a><span class="classifier">ndarray of shape (n_features,)</span></dt><dd><p>Return impurity-based feature importances (the higher, the more
important the feature).</p>
<p>Warning: impurity-based feature importances can be misleading for
high cardinality features (many unique values). See
<code class="xref py py-func docutils literal notranslate"><span class="pre">sklearn.inspection.permutation_importance()</span></code> as an alternative.</p>
</dd>
<dt><a href="#id22"><span class="problematic" id="id23">tree_</span></a><span class="classifier">Tree</span></dt><dd><p>The underlying Tree object. Please refer to
<code class="docutils literal notranslate"><span class="pre">help(sklearn.tree._tree.Tree)</span></code> for attributes of Tree object and
<span class="xref std std-ref">sphx_glr_auto_examples_tree_plot_unveil_tree_structure.py</span>
for basic usage of these attributes.</p>
</dd>
</dl>
<p>ExtraTreeClassifier : An extremely randomized tree classifier.
sklearn.ensemble.ExtraTreesClassifier : An extra-trees classifier.
sklearn.ensemble.ExtraTreesRegressor : An extra-trees regressor.</p>
<p>The default values for the parameters controlling the size of the trees
(e.g. <code class="docutils literal notranslate"><span class="pre">max_depth</span></code>, <code class="docutils literal notranslate"><span class="pre">min_samples_leaf</span></code>, etc.) lead to fully grown and
unpruned trees which can potentially be very large on some data sets. To
reduce memory consumption, the complexity and size of the trees should be
controlled by setting those parameter values.</p>
<dl class="footnote brackets">
<dt class="label" id="id1"><span class="brackets">1</span></dt>
<dd><p>P. Geurts, D. Ernst., and L. Wehenkel, “Extremely randomized trees”,
Machine Learning, 63(1), 3-42, 2006.</p>
</dd>
</dl>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_diabetes</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">BaggingRegressor</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">ExtraTreeRegressor</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">load_diabetes</span><span class="p">(</span><span class="n">return_X_y</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span>
<span class="gp">... </span>    <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">extra_tree</span> <span class="o">=</span> <span class="n">ExtraTreeRegressor</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">reg</span> <span class="o">=</span> <span class="n">BaggingRegressor</span><span class="p">(</span><span class="n">extra_tree</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span>
<span class="gp">... </span>    <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">reg</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
<span class="go">0.33...</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="stpredictions.models.OK3.OK3Regressor">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">stpredictions.models.OK3.</span></span><span class="sig-name descname"><span class="pre">OK3Regressor</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">criterion</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'mse'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">splitter</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'best'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_depth</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_samples_split</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_samples_leaf</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_weight_fraction_leaf</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_features</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">random_state</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_leaf_nodes</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_impurity_decrease</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_impurity_split</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ccp_alpha</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kernel</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'linear'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#stpredictions.models.OK3.OK3Regressor" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="#stpredictions.models.OK3.BaseKernelizedOutputTree" title="stpredictions.models.OK3._classes.BaseKernelizedOutputTree"><code class="xref py py-class docutils literal notranslate"><span class="pre">stpredictions.models.OK3._classes.BaseKernelizedOutputTree</span></code></a></p>
<p>A decision tree regressor for the OK3 method.</p>
<dl>
<dt>criterion<span class="classifier">{“mse”}, default=”mse”</span></dt><dd><p>The function to measure the quality of a split. Supported criteria
are “mse” for the mean squared error, which is equal to variance
reduction as feature selection criterion and minimizes the L2 loss
using the mean of each terminal node, “friedman_mse”, which uses mean
squared error with Friedman’s improvement score for potential splits,
and “mae” for the mean absolute error, which minimizes the L1 loss
using the median of each terminal node.</p>
</dd>
<dt>splitter<span class="classifier">{“best”, “random”}, default=”best”</span></dt><dd><p>The strategy used to choose the split at each node. Supported
strategies are “best” to choose the best split and “random” to choose
the best random split.</p>
</dd>
<dt>max_depth<span class="classifier">int, default=None</span></dt><dd><p>The maximum depth of the tree. If None, then nodes are expanded until
all leaves are pure or until all leaves contain less than
min_samples_split samples.</p>
</dd>
<dt>min_samples_split<span class="classifier">int or float, default=2</span></dt><dd><p>The minimum number of samples required to split an internal node:</p>
<ul class="simple">
<li><p>If int, then consider <cite>min_samples_split</cite> as the minimum number.</p></li>
<li><p>If float, then <cite>min_samples_split</cite> is a fraction and
<cite>ceil(min_samples_split * n_samples)</cite> are the minimum
number of samples for each split.</p></li>
</ul>
</dd>
<dt>min_samples_leaf<span class="classifier">int or float, default=1</span></dt><dd><p>The minimum number of samples required to be at a leaf node.
A split point at any depth will only be considered if it leaves at
least <code class="docutils literal notranslate"><span class="pre">min_samples_leaf</span></code> training samples in each of the left and
right branches.  This may have the effect of smoothing the model,
especially in regression.</p>
<ul class="simple">
<li><p>If int, then consider <cite>min_samples_leaf</cite> as the minimum number.</p></li>
<li><p>If float, then <cite>min_samples_leaf</cite> is a fraction and
<cite>ceil(min_samples_leaf * n_samples)</cite> are the minimum
number of samples for each node.</p></li>
</ul>
</dd>
<dt>min_weight_fraction_leaf<span class="classifier">float, default=0.0</span></dt><dd><p>The minimum weighted fraction of the sum total of weights (of all
the input samples) required to be at a leaf node. Samples have
equal weight when sample_weight is not provided.</p>
</dd>
<dt>max_features<span class="classifier">int, float or {“auto”, “sqrt”, “log2”}, default=None</span></dt><dd><p>The number of features to consider when looking for the best split:</p>
<ul class="simple">
<li><p>If int, then consider <cite>max_features</cite> features at each split.</p></li>
<li><p>If float, then <cite>max_features</cite> is a fraction and
<cite>int(max_features * n_features)</cite> features are considered at each
split.</p></li>
<li><p>If “auto”, then <cite>max_features=n_features</cite>.</p></li>
<li><p>If “sqrt”, then <cite>max_features=sqrt(n_features)</cite>.</p></li>
<li><p>If “log2”, then <cite>max_features=log2(n_features)</cite>.</p></li>
<li><p>If None, then <cite>max_features=n_features</cite>.</p></li>
</ul>
<p>Note: the search for a split does not stop until at least one
valid partition of the node samples is found, even if it requires to
effectively inspect more than <code class="docutils literal notranslate"><span class="pre">max_features</span></code> features.</p>
</dd>
<dt>random_state<span class="classifier">int, RandomState instance, default=None</span></dt><dd><p>Controls the randomness of the estimator. The features are always
randomly permuted at each split, even if <code class="docutils literal notranslate"><span class="pre">splitter</span></code> is set to
<code class="docutils literal notranslate"><span class="pre">&quot;best&quot;</span></code>. When <code class="docutils literal notranslate"><span class="pre">max_features</span> <span class="pre">&lt;</span> <span class="pre">n_features</span></code>, the algorithm will
select <code class="docutils literal notranslate"><span class="pre">max_features</span></code> at random at each split before finding the best
split among them. But the best found split may vary across different
runs, even if <code class="docutils literal notranslate"><span class="pre">max_features=n_features</span></code>. That is the case, if the
improvement of the criterion is identical for several splits and one
split has to be selected at random. To obtain a deterministic behaviour
during fitting, <code class="docutils literal notranslate"><span class="pre">random_state</span></code> has to be fixed to an integer.
See <span class="xref std std-term">Glossary</span> for details.</p>
</dd>
<dt>max_leaf_nodes<span class="classifier">int, default=None</span></dt><dd><p>Grow a tree with <code class="docutils literal notranslate"><span class="pre">max_leaf_nodes</span></code> in best-first fashion.
Best nodes are defined as relative reduction in impurity.
If None then unlimited number of leaf nodes.</p>
</dd>
<dt>min_impurity_decrease<span class="classifier">float, default=0.0</span></dt><dd><p>A node will be split if this split induces a decrease of the impurity
greater than or equal to this value.</p>
<p>The weighted impurity decrease equation is the following:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">N_t</span> <span class="o">/</span> <span class="n">N</span> <span class="o">*</span> <span class="p">(</span><span class="n">impurity</span> <span class="o">-</span> <span class="n">N_t_R</span> <span class="o">/</span> <span class="n">N_t</span> <span class="o">*</span> <span class="n">right_impurity</span>
                    <span class="o">-</span> <span class="n">N_t_L</span> <span class="o">/</span> <span class="n">N_t</span> <span class="o">*</span> <span class="n">left_impurity</span><span class="p">)</span>
</pre></div>
</div>
<p>where <code class="docutils literal notranslate"><span class="pre">N</span></code> is the total number of samples, <code class="docutils literal notranslate"><span class="pre">N_t</span></code> is the number of
samples at the current node, <code class="docutils literal notranslate"><span class="pre">N_t_L</span></code> is the number of samples in the
left child, and <code class="docutils literal notranslate"><span class="pre">N_t_R</span></code> is the number of samples in the right child.</p>
<p><code class="docutils literal notranslate"><span class="pre">N</span></code>, <code class="docutils literal notranslate"><span class="pre">N_t</span></code>, <code class="docutils literal notranslate"><span class="pre">N_t_R</span></code> and <code class="docutils literal notranslate"><span class="pre">N_t_L</span></code> all refer to the weighted sum,
if <code class="docutils literal notranslate"><span class="pre">sample_weight</span></code> is passed.</p>
</dd>
<dt>min_impurity_split<span class="classifier">float, (default=0)</span></dt><dd><p>Threshold for early stopping in tree growth. A node will split
if its impurity is above the threshold, otherwise it is a leaf.</p>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version 0.19: </span><code class="docutils literal notranslate"><span class="pre">min_impurity_split</span></code> has been deprecated in favor of
<code class="docutils literal notranslate"><span class="pre">min_impurity_decrease</span></code> in 0.19. The default value of
<code class="docutils literal notranslate"><span class="pre">min_impurity_split</span></code> has changed from 1e-7 to 0 in 0.23 and it
will be removed in 0.25. Use <code class="docutils literal notranslate"><span class="pre">min_impurity_decrease</span></code> instead.</p>
</div>
</dd>
<dt>ccp_alpha<span class="classifier">non-negative float, default=0.0</span></dt><dd><p>Complexity parameter used for Minimal Cost-Complexity Pruning. The
subtree with the largest cost complexity that is smaller than
<code class="docutils literal notranslate"><span class="pre">ccp_alpha</span></code> will be chosen. By default, no pruning is performed. See
<span class="xref std std-ref">minimal_cost_complexity_pruning</span> for details.</p>
</dd>
<dt>kernel<span class="classifier">string, or tuple (string, params) or instance of the Kernel class, default=”linear”</span></dt><dd><p>The type of kernel to use to compare the output data. Changing this
parameter changes also implicitely the nature of the Hilbert space
in which the output data are embedded.
The string describes the type of Kernel to use (defined in Kernel.py), 
The optional params given are here to set particular parameters values
for the chosen kernel type.</p>
</dd>
</dl>
<dl>
<dt><a href="#id24"><span class="problematic" id="id25">feature_importances_</span></a><span class="classifier">ndarray of shape (n_features,)</span></dt><dd><p>The feature importances.
The higher, the more important the feature.
The importance of a feature is computed as the
(normalized) total reduction of the criterion brought
by that feature. It is also known as the Gini importance <a href="#id26"><span class="problematic" id="id2">[4]_</span></a>.</p>
<p>Warning: impurity-based feature importances can be misleading for
high cardinality features (many unique values). See
<code class="xref py py-func docutils literal notranslate"><span class="pre">sklearn.inspection.permutation_importance()</span></code> as an alternative.</p>
</dd>
<dt><a href="#id27"><span class="problematic" id="id28">max_features_</span></a><span class="classifier">int</span></dt><dd><p>The inferred value of max_features.</p>
</dd>
<dt><a href="#id29"><span class="problematic" id="id30">n_features_</span></a><span class="classifier">int</span></dt><dd><p>The number of features when <code class="docutils literal notranslate"><span class="pre">fit</span></code> is performed.</p>
</dd>
<dt><a href="#id31"><span class="problematic" id="id32">tree_</span></a><span class="classifier">Tree</span></dt><dd><p>The underlying Tree object. Please refer to
<code class="docutils literal notranslate"><span class="pre">help(sklearn.tree._tree.Tree)</span></code> for attributes of Tree object and
<span class="xref std std-ref">sphx_glr_auto_examples_tree_plot_unveil_tree_structure.py</span>
for basic usage of these attributes.</p>
</dd>
<dt>leaves_preds<span class="classifier">array of shape (n_nodes, n_components),</span></dt><dd><p>where n_nodes is the number of nodes of the grown tree and
n_components is the number of values used to represent an output.</p>
<p>This array stores for each leaf of the tree, the decoded predictions in Y.</p>
</dd>
</dl>
<p>The default values for the parameters controlling the size of the trees
(e.g. <code class="docutils literal notranslate"><span class="pre">max_depth</span></code>, <code class="docutils literal notranslate"><span class="pre">min_samples_leaf</span></code>, etc.) lead to fully grown and
unpruned trees which can potentially be very large on some data sets. To
reduce memory consumption, the complexity and size of the trees should be
controlled by setting those parameter values.</p>
<dl class="footnote brackets">
<dt class="label" id="id3"><span class="brackets">1</span></dt>
<dd><p>Pierre Geurts, Louis Wehenkel, Florence d’Alché-Buc. 
“Kernelizing the output of tree-based methods.”
Proc.  of the 23rd International Conference on Machine Learning, 
2006, United States.  pp.345–352,￿10.1145/1143844.1143888￿.</p>
</dd>
</dl>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span>&gt;&gt;&gt; from sklearn.datasets import load_diabetes
&gt;&gt;&gt; from sklearn.model_selection import cross_val_score
&gt;&gt;&gt; from ??? import OK3Regressor
&gt;&gt;&gt; X, y = load_diabetes(return_X_y=True)
&gt;&gt;&gt; regressor = OK3Regressor(random_state=0)
&gt;&gt;&gt; cross_val_score(regressor, X, y, cv=10)
...                    
...
array([-0.39..., -0.46...,  0.02...,  0.06..., -0.50...,
       0.16...,  0.11..., -0.73..., -0.30..., -0.00...])
</pre></div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="stpredictions.models.OK3.OK3Regressor.fit">
<span class="sig-name descname"><span class="pre">fit</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sample_weight</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">check_input</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">X_idx_sorted</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'deprecated'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kernel</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">in_ensemble</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">Gram_y</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#stpredictions.models.OK3.OK3Regressor.fit" title="Permalink to this definition"></a></dt>
<dd><p>Build a decision tree regressor from the training set (X, y).</p>
<dl class="simple">
<dt>X<span class="classifier">{array-like, sparse matrix} of shape (n_samples, n_features)</span></dt><dd><p>The training input samples. Internally, it will be converted to
<code class="docutils literal notranslate"><span class="pre">dtype=np.float32</span></code> and if a sparse matrix is provided
to a sparse <code class="docutils literal notranslate"><span class="pre">csc_matrix</span></code>.</p>
</dd>
<dt>y<span class="classifier">array-like of shape (n_samples,) or (n_samples, n_outputs)</span></dt><dd><p>The target values (real numbers). Use <code class="docutils literal notranslate"><span class="pre">dtype=np.float64</span></code> and
<code class="docutils literal notranslate"><span class="pre">order='C'</span></code> for maximum efficiency.</p>
</dd>
<dt>sample_weight<span class="classifier">array-like of shape (n_samples,), default=None</span></dt><dd><p>Sample weights. If None, then samples are equally weighted. Splits
that would create child nodes with net zero or negative weight are
ignored while searching for a split in each node.</p>
</dd>
<dt>check_input<span class="classifier">bool, default=True</span></dt><dd><p>Allow to bypass several input checking.
Don’t use this parameter unless you know what you do.</p>
</dd>
<dt>X_idx_sorted<span class="classifier">deprecated, default=”deprecated”</span></dt><dd><p>This parameter is deprecated and has no effect.
It will be removed in v0.26.</p>
</dd>
<dt>kernel<span class="classifier">string, or tuple (string, params) or instance of the Kernel class, default=”linear”</span></dt><dd><p>The type of kernel to use to compare the output data. Changing this
parameter changes also implicitely the nature of the Hilbert space
in which the output data are embedded.
The string describes the type of Kernel to use (defined in Kernel.py), 
The optional params given are here to set particular parameters values
for the chosen kernel type.
This parameter can be set also here in the fit method instead of __init__.</p>
</dd>
</dl>
<dl class="simple">
<dt>self<span class="classifier">OK3Regressor</span></dt><dd><p>Fitted estimator.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="stpredictions.models.OK3.OKForestRegressor">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">stpredictions.models.OK3.</span></span><span class="sig-name descname"><span class="pre">OKForestRegressor</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">base_estimator</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_estimators</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">100</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">estimator_params</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">()</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bootstrap</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">oob_score</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_jobs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">random_state</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">warm_start</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_samples</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kernel</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'linear'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#stpredictions.models.OK3.OKForestRegressor" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="#stpredictions.models.OK3.BaseOKForest" title="stpredictions.models.OK3._forest.BaseOKForest"><code class="xref py py-class docutils literal notranslate"><span class="pre">stpredictions.models.OK3._forest.BaseOKForest</span></code></a></p>
<p>Base class for forest of ok-trees-based regressors.</p>
<p>Warning: This class should not be used directly. Use derived classes
instead.</p>
<dl class="py method">
<dt class="sig sig-object py" id="stpredictions.models.OK3.OKForestRegressor.predict">
<span class="sig-name descname"><span class="pre">predict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">candidates</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_top_k</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">precomputed_weights</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#stpredictions.models.OK3.OKForestRegressor.predict" title="Permalink to this definition"></a></dt>
<dd><p>Predict structured objects for X.</p>
<p>The predicted structured objects based on X are returned.
Performs an argmin research algorithm amongst the possible outputs</p>
<dl class="simple">
<dt>X<span class="classifier">{array-like, sparse matrix} of shape (n_samples, n_features)</span></dt><dd><p>The input samples. Internally, it will be converted to
<code class="docutils literal notranslate"><span class="pre">dtype=np.float32</span></code> and if a sparse matrix is provided
to a sparse <code class="docutils literal notranslate"><span class="pre">csr_matrix</span></code>.</p>
</dd>
<dt>candidates<span class="classifier">array of shape (nb_candidates, vectorial_repr_len), default=None</span></dt><dd><p>The candidates outputs for the minimisation problem of decoding the predictions
in the Hilbert space. 
If not given or None, it will be set to the output training matrix.</p>
</dd>
<dt>check_input<span class="classifier">bool, default=True</span></dt><dd><p>Allow to bypass several input checking.
Don’t use this parameter unless you know what you do.</p>
</dd>
<dt>return_top_k<span class="classifier">int, default=1</span></dt><dd><p>Indicate how many decoded outputs to return for each example (or for each leaf).
Default is one : select the output that gives the  minimum “distance” with the 
predicted value in the Hilbert space. 
But it is useful to be able to return for example the 5 best candidates in order 
to evaluate a top 5 accuracy metric.</p>
</dd>
</dl>
<dl class="simple">
<dt>output<span class="classifier"></span></dt><dd><p>array of shape (n_samples, vectorial_repr_len) containing the vectorial 
representations of the structured output objects (found in the set of candidates, 
or if it is not given, among the training outputs).</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stpredictions.models.OK3.OKForestRegressor.predict_weights">
<span class="sig-name descname"><span class="pre">predict_weights</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#stpredictions.models.OK3.OKForestRegressor.predict_weights" title="Permalink to this definition"></a></dt>
<dd><p>Predict weights (on the training samples) for X.</p>
<p>The predicted weights of an input sample are computed as the
mean predicted weights of the trees in the forest.</p>
<dl class="simple">
<dt>X<span class="classifier">{array-like, sparse matrix} of shape (n_samples, n_features)</span></dt><dd><p>The input samples. Internally, its dtype will be converted to
<code class="docutils literal notranslate"><span class="pre">dtype=np.float32</span></code>. If a sparse matrix is provided, it will be
converted into a sparse <code class="docutils literal notranslate"><span class="pre">csr_matrix</span></code>.</p>
</dd>
</dl>
<p>A (X.shape[0], n_training_samples) array which gives for each test example (line number)
and for each training sample its weight in the node (O if the sample doesn’t fall
in any of the same leaves as the test example, and a non-negative value depending on ‘sample_weight’ otherwise.)</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stpredictions.models.OK3.OKForestRegressor.r2_score_in_Hilbert">
<span class="sig-name descname"><span class="pre">r2_score_in_Hilbert</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sample_weight</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#stpredictions.models.OK3.OKForestRegressor.r2_score_in_Hilbert" title="Permalink to this definition"></a></dt>
<dd><p>Calcule le score R2 SANS décodage</p>
<p>Return the coefficient of determination R^2 of the prediction in the Hilbert space.
The coefficient R^2 is defined as (1 - u/v), where u is the residual
sum of squares ((y_true - y_pred) ** 2).sum() and v is the total
sum of squares ((y_true - y_true.mean()) ** 2).sum().</p>
<dl class="simple">
<dt>X<span class="classifier">array-like of shape (n_samples, n_features)</span></dt><dd><p>Test samples.</p>
</dd>
<dt>y<span class="classifier">array-like of shape (n_samples,) or (n_samples, n_outputs)</span></dt><dd><p>True outputs for X.</p>
</dd>
<dt>sample_weight<span class="classifier">array-like of shape (n_samples,), default=None</span></dt><dd><p>Sample weights.</p>
</dd>
</dl>
<dl class="simple">
<dt>score<span class="classifier">float</span></dt><dd><p>R2 score of the predictions in the Hilbert space wrt. the embedded values of y.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stpredictions.models.OK3.OKForestRegressor.score">
<span class="sig-name descname"><span class="pre">score</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">candidates</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">metric</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'accuracy'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sample_weight</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">precomputed_weights</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#stpredictions.models.OK3.OKForestRegressor.score" title="Permalink to this definition"></a></dt>
<dd><p>Calcule le score après décodage</p>
<p>Return either</p>
<blockquote>
<div><p>-the coefficient of determination R^2 of the prediction.
The coefficient R^2 is defined as (1 - u/v), where u is the residual
sum of squares ((y_true - y_pred) ** 2).sum() and v is the total
sum of squares ((y_true - y_true.mean()) ** 2).sum().
(IF self.kernel=”mse_reg”)</p>
<p>-the mean accuracy of the predictions if metric=”accuracy”.
(Requires that all labels match to count as positive in case of multilabel)</p>
<p>-the mean hamming score of the predictions if metric=”hamming”
(Well suited for multilabel classification)</p>
<p>-the mean top k accuracy score if metric=”<a href="#id33"><span class="problematic" id="id34">top_</span></a>”+str(k)
It works with all wanted value of k.</p>
</div></blockquote>
<p>It is possible to set the ‘sample_weight’ parameter for all these metrics.</p>
<p>All this score metrics are highly dependent on the candidates set because it
deals with the decoded predictions (which are among this set).
If you want to compute a score only based on the tree structure, you can
use the following method ‘r2_score_in_Hilbert’.</p>
<dl class="simple">
<dt>X<span class="classifier">array-like of shape (n_samples, n_features)</span></dt><dd><p>Test samples.</p>
</dd>
<dt>y<span class="classifier">array-like of shape (n_samples,) or (n_samples, n_outputs)</span></dt><dd><p>True outputs for X.</p>
</dd>
<dt>candidates<span class="classifier">array-like of shape (nb_candidates, n_outputs)</span></dt><dd><p>Possible decoded outputs for X</p>
</dd>
<dt>metric<span class="classifier">str, default=”accuracy”</span></dt><dd><p>The way to compute the score</p>
</dd>
<dt>sample_weight<span class="classifier">array-like of shape (n_samples,), default=None</span></dt><dd><p>Sample weights.</p>
</dd>
</dl>
<dl class="simple">
<dt>score<span class="classifier">float</span></dt><dd><p>Chosen score between self.predict(X) and y.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="stpredictions.models.OK3.RandomOKForestRegressor">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">stpredictions.models.OK3.</span></span><span class="sig-name descname"><span class="pre">RandomOKForestRegressor</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">n_estimators</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">100</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">criterion</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'mse'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_depth</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_samples_split</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_samples_leaf</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_weight_fraction_leaf</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_features</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'auto'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_leaf_nodes</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_impurity_decrease</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_impurity_split</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bootstrap</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">oob_score</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_jobs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">random_state</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">warm_start</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ccp_alpha</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_samples</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kernel</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'linear'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#stpredictions.models.OK3.RandomOKForestRegressor" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="#stpredictions.models.OK3.OKForestRegressor" title="stpredictions.models.OK3._forest.OKForestRegressor"><code class="xref py py-class docutils literal notranslate"><span class="pre">stpredictions.models.OK3._forest.OKForestRegressor</span></code></a></p>
<p>A random ok-forest regressor.</p>
<p>A random forest is a meta estimator that fits a number of
decision trees on various sub-samples of the dataset and uses averaging
to improve the predictive accuracy and control over-fitting.
The sub-sample size is controlled with the <cite>max_samples</cite> parameter if
<cite>bootstrap=True</cite> (default), otherwise the whole dataset is used to build
each tree.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<dl>
<dt>n_estimators<span class="classifier">int, default=100</span></dt><dd><p>The number of trees in the forest.</p>
</dd>
<dt>criterion<span class="classifier">{“mse”}, default=”mse”</span></dt><dd><p>The function to measure the quality of a split. Supported criteria
are “mse” for the mean squared error, which is equal to variance
reduction as feature selection criterion</p>
</dd>
<dt>max_depth<span class="classifier">int, default=None</span></dt><dd><p>The maximum depth of the tree. If None, then nodes are expanded until
all leaves are pure or until all leaves contain less than
min_samples_split samples.</p>
</dd>
<dt>min_samples_split<span class="classifier">int or float, default=2</span></dt><dd><p>The minimum number of samples required to split an internal node:</p>
<ul class="simple">
<li><p>If int, then consider <cite>min_samples_split</cite> as the minimum number.</p></li>
<li><p>If float, then <cite>min_samples_split</cite> is a fraction and
<cite>ceil(min_samples_split * n_samples)</cite> are the minimum
number of samples for each split.</p></li>
</ul>
<div class="versionchanged">
<p><span class="versionmodified changed">Changed in version 0.18: </span>Added float values for fractions.</p>
</div>
</dd>
<dt>min_samples_leaf<span class="classifier">int or float, default=1</span></dt><dd><p>The minimum number of samples required to be at a leaf node.
A split point at any depth will only be considered if it leaves at
least <code class="docutils literal notranslate"><span class="pre">min_samples_leaf</span></code> training samples in each of the left and
right branches.  This may have the effect of smoothing the model,
especially in regression.</p>
<ul class="simple">
<li><p>If int, then consider <cite>min_samples_leaf</cite> as the minimum number.</p></li>
<li><p>If float, then <cite>min_samples_leaf</cite> is a fraction and
<cite>ceil(min_samples_leaf * n_samples)</cite> are the minimum
number of samples for each node.</p></li>
</ul>
<div class="versionchanged">
<p><span class="versionmodified changed">Changed in version 0.18: </span>Added float values for fractions.</p>
</div>
</dd>
<dt>min_weight_fraction_leaf<span class="classifier">float, default=0.0</span></dt><dd><p>The minimum weighted fraction of the sum total of weights (of all
the input samples) required to be at a leaf node. Samples have
equal weight when sample_weight is not provided.</p>
</dd>
<dt>max_features<span class="classifier">{“auto”, “sqrt”, “log2”}, int or float, default=”auto”</span></dt><dd><p>The number of features to consider when looking for the best split:</p>
<ul class="simple">
<li><p>If int, then consider <cite>max_features</cite> features at each split.</p></li>
<li><p>If float, then <cite>max_features</cite> is a fraction and
<cite>round(max_features * n_features)</cite> features are considered at each
split.</p></li>
<li><p>If “auto”, then <cite>max_features=n_features</cite>.</p></li>
<li><p>If “sqrt”, then <cite>max_features=sqrt(n_features)</cite>.</p></li>
<li><p>If “log2”, then <cite>max_features=log2(n_features)</cite>.</p></li>
<li><p>If None, then <cite>max_features=n_features</cite>.</p></li>
</ul>
<p>Note: the search for a split does not stop until at least one
valid partition of the node samples is found, even if it requires to
effectively inspect more than <code class="docutils literal notranslate"><span class="pre">max_features</span></code> features.</p>
</dd>
<dt>max_leaf_nodes<span class="classifier">int, default=None</span></dt><dd><p>Grow trees with <code class="docutils literal notranslate"><span class="pre">max_leaf_nodes</span></code> in best-first fashion.
Best nodes are defined as relative reduction in impurity.
If None then unlimited number of leaf nodes.</p>
</dd>
<dt>min_impurity_decrease<span class="classifier">float, default=0.0</span></dt><dd><p>A node will be split if this split induces a decrease of the impurity
greater than or equal to this value.</p>
<p>The weighted impurity decrease equation is the following:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">N_t</span> <span class="o">/</span> <span class="n">N</span> <span class="o">*</span> <span class="p">(</span><span class="n">impurity</span> <span class="o">-</span> <span class="n">N_t_R</span> <span class="o">/</span> <span class="n">N_t</span> <span class="o">*</span> <span class="n">right_impurity</span>
                    <span class="o">-</span> <span class="n">N_t_L</span> <span class="o">/</span> <span class="n">N_t</span> <span class="o">*</span> <span class="n">left_impurity</span><span class="p">)</span>
</pre></div>
</div>
<p>where <code class="docutils literal notranslate"><span class="pre">N</span></code> is the total number of samples, <code class="docutils literal notranslate"><span class="pre">N_t</span></code> is the number of
samples at the current node, <code class="docutils literal notranslate"><span class="pre">N_t_L</span></code> is the number of samples in the
left child, and <code class="docutils literal notranslate"><span class="pre">N_t_R</span></code> is the number of samples in the right child.</p>
<p><code class="docutils literal notranslate"><span class="pre">N</span></code>, <code class="docutils literal notranslate"><span class="pre">N_t</span></code>, <code class="docutils literal notranslate"><span class="pre">N_t_R</span></code> and <code class="docutils literal notranslate"><span class="pre">N_t_L</span></code> all refer to the weighted sum,
if <code class="docutils literal notranslate"><span class="pre">sample_weight</span></code> is passed.</p>
</dd>
<dt>min_impurity_split<span class="classifier">float, default=None</span></dt><dd><p>Threshold for early stopping in tree growth. A node will split
if its impurity is above the threshold, otherwise it is a leaf.</p>
</dd>
<dt>bootstrap<span class="classifier">bool, default=True</span></dt><dd><p>Whether bootstrap samples are used when building trees. If False, the
whole dataset is used to build each tree.</p>
</dd>
<dt>oob_score<span class="classifier">bool, default=False</span></dt><dd><p>whether to use out-of-bag samples to estimate
the R^2 on unseen data.</p>
</dd>
<dt>n_jobs<span class="classifier">int, default=None</span></dt><dd><p>The number of jobs to run in parallel. <code class="xref py py-meth docutils literal notranslate"><span class="pre">fit()</span></code>, <code class="xref py py-meth docutils literal notranslate"><span class="pre">predict()</span></code>,
<code class="xref py py-meth docutils literal notranslate"><span class="pre">decision_path()</span></code> and <code class="xref py py-meth docutils literal notranslate"><span class="pre">apply()</span></code> are all parallelized over the
trees. <code class="docutils literal notranslate"><span class="pre">None</span></code> means 1 unless in a <code class="xref py py-obj docutils literal notranslate"><span class="pre">joblib.parallel_backend</span></code>
context. <code class="docutils literal notranslate"><span class="pre">-1</span></code> means using all processors. See <span class="xref std std-term">Glossary</span> for more details.</p>
</dd>
<dt>random_state<span class="classifier">int or RandomState, default=None</span></dt><dd><p>Controls both the randomness of the bootstrapping of the samples used
when building trees (if <code class="docutils literal notranslate"><span class="pre">bootstrap=True</span></code>) and the sampling of the
features to consider when looking for the best split at each node
(if <code class="docutils literal notranslate"><span class="pre">max_features</span> <span class="pre">&lt;</span> <span class="pre">n_features</span></code>).
See <span class="xref std std-term">Glossary</span> for details.</p>
</dd>
<dt>verbose<span class="classifier">int, default=0</span></dt><dd><p>Controls the verbosity when fitting and predicting.</p>
</dd>
<dt>warm_start<span class="classifier">bool, default=False</span></dt><dd><p>When set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, reuse the solution of the previous call to fit
and add more estimators to the ensemble, otherwise, just fit a whole
new forest. See <span class="xref std std-term">the Glossary</span>.</p>
</dd>
<dt>ccp_alpha<span class="classifier">non-negative float, default=0.0</span></dt><dd><p>Complexity parameter used for Minimal Cost-Complexity Pruning. The
subtree with the largest cost complexity that is smaller than
<code class="docutils literal notranslate"><span class="pre">ccp_alpha</span></code> will be chosen. By default, no pruning is performed. See
<span class="xref std std-ref">minimal_cost_complexity_pruning</span> for details.</p>
</dd>
<dt>max_samples<span class="classifier">int or float, default=None</span></dt><dd><p>If bootstrap is True, the number of samples to draw from X
to train each base estimator.</p>
<ul class="simple">
<li><p>If None (default), then draw <cite>X.shape[0]</cite> samples.</p></li>
<li><p>If int, then draw <cite>max_samples</cite> samples.</p></li>
<li><p>If float, then draw <cite>max_samples * X.shape[0]</cite> samples. Thus,
<cite>max_samples</cite> should be in the interval <cite>(0, 1)</cite>.</p></li>
</ul>
</dd>
</dl>
<dl>
<dt><a href="#id35"><span class="problematic" id="id36">base_estimator_</span></a><span class="classifier">DecisionTreeRegressor</span></dt><dd><p>The child estimator template used to create the collection of fitted
sub-estimators.</p>
</dd>
<dt><a href="#id37"><span class="problematic" id="id38">estimators_</span></a><span class="classifier">list of DecisionTreeRegressor</span></dt><dd><p>The collection of fitted sub-estimators.</p>
</dd>
<dt><a href="#id39"><span class="problematic" id="id40">feature_importances_</span></a><span class="classifier">ndarray of shape (n_features,)</span></dt><dd><p>The impurity-based feature importances.
The higher, the more important the feature.
The importance of a feature is computed as the (normalized)
total reduction of the criterion brought by that feature.  It is also
known as the Gini importance.</p>
<p>Warning: impurity-based feature importances can be misleading for
high cardinality features (many unique values). See
<code class="xref py py-func docutils literal notranslate"><span class="pre">sklearn.inspection.permutation_importance()</span></code> as an alternative.</p>
</dd>
<dt><a href="#id41"><span class="problematic" id="id42">n_features_</span></a><span class="classifier">int</span></dt><dd><p>The number of features when <code class="docutils literal notranslate"><span class="pre">fit</span></code> is performed.</p>
</dd>
<dt><a href="#id43"><span class="problematic" id="id44">n_outputs_</span></a><span class="classifier">int</span></dt><dd><p>The number of outputs when <code class="docutils literal notranslate"><span class="pre">fit</span></code> is performed.</p>
</dd>
<dt><a href="#id45"><span class="problematic" id="id46">oob_score_</span></a><span class="classifier">float</span></dt><dd><p>Score of the training dataset obtained using an out-of-bag estimate.
This attribute exists only when <code class="docutils literal notranslate"><span class="pre">oob_score</span></code> is True.</p>
</dd>
<dt><a href="#id47"><span class="problematic" id="id48">oob_prediction_</span></a><span class="classifier">ndarray of shape (n_samples,)</span></dt><dd><p>Prediction computed with out-of-bag estimate on the training set.
This attribute exists only when <code class="docutils literal notranslate"><span class="pre">oob_score</span></code> is True.</p>
</dd>
</dl>
<p>OK3Regressor, ExtraOKTreesRegressor</p>
<p>The default values for the parameters controlling the size of the trees
(e.g. <code class="docutils literal notranslate"><span class="pre">max_depth</span></code>, <code class="docutils literal notranslate"><span class="pre">min_samples_leaf</span></code>, etc.) lead to fully grown and
unpruned trees which can potentially be very large on some data sets. To
reduce memory consumption, the complexity and size of the trees should be
controlled by setting those parameter values.</p>
<p>The features are always randomly permuted at each split. Therefore,
the best found split may vary, even with the same training data,
<code class="docutils literal notranslate"><span class="pre">max_features=n_features</span></code> and <code class="docutils literal notranslate"><span class="pre">bootstrap=False</span></code>, if the improvement
of the criterion is identical for several splits enumerated during the
search of the best split. To obtain a deterministic behaviour during
fitting, <code class="docutils literal notranslate"><span class="pre">random_state</span></code> has to be fixed.</p>
<p>The default value <code class="docutils literal notranslate"><span class="pre">max_features=&quot;auto&quot;</span></code> uses <code class="docutils literal notranslate"><span class="pre">n_features</span></code>
rather than <code class="docutils literal notranslate"><span class="pre">n_features</span> <span class="pre">/</span> <span class="pre">3</span></code>. The latter was originally suggested in
[1], whereas the former was more recently justified empirically in [2].</p>
<dl class="footnote brackets">
<dt class="label" id="id4"><span class="brackets">1</span></dt>
<dd><ol class="upperalpha simple" start="12">
<li><p>Breiman, “Random Forests”, Machine Learning, 45(1), 5-32, 2001.</p></li>
</ol>
</dd>
<dt class="label" id="id5"><span class="brackets">2</span></dt>
<dd><p>P. Geurts, D. Ernst., and L. Wehenkel, “Extremely randomized
trees”, Machine Learning, 63(1), 3-42, 2006.</p>
</dd>
</dl>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestRegressor</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_regression</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_regression</span><span class="p">(</span><span class="n">n_features</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">n_informative</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
<span class="gp">... </span>                       <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">regr</span> <span class="o">=</span> <span class="n">RandomOKForestRegressor</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">kernel</span><span class="o">=</span><span class="s2">&quot;linear&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">regr</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="go">RandomForestRegressor(...)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">regr</span><span class="o">.</span><span class="n">predict</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]]))</span>
<span class="go">[-8.32987858]</span>
</pre></div>
</div>
</dd></dl>

</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="stpredictions.models.IOKR.html" class="btn btn-neutral float-left" title="stpredictions.models.IOKR package" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="stpredictions.models.OK3.tests.html" class="btn btn-neutral float-right" title="stpredictions.models.OK3.tests package" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, To Fullfill.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>